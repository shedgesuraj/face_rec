{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayus\\AppData\\Local\\Temp\\ipykernel_1764\\246831167.py:11: NeptuneDeprecationWarning: `init` is deprecated, use `init_run` instead. We'll end support of it in `neptune-client==1.0.0`.\n",
      "  run = neptune.init(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/aayushpandey616/yellowcap/e/YEL-1\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "[INFO] serializing 25 encodings...\n",
      "[INFO] training model...\n",
      "Sample = 52\n",
      "Train Percentage = 50.00%\n",
      "Gross Accuracy = 85.10%\n",
      "Net Accuracy = 63.83%\n",
      "Detection Rate = 75.00%\n",
      "\n",
      "[INFO] serializing 538.6299999999999 encodings...\n",
      "[INFO] training model...\n",
      "Sample = 52\n",
      "Train Percentage = 60.00%\n",
      "Gross Accuracy = 80.86%\n",
      "Net Accuracy = 80.86%\n",
      "Detection Rate = 100.00%\n",
      "\n",
      "[INFO] serializing 272.57 encodings...\n",
      "[INFO] training model...\n",
      "Sample = 52\n",
      "Train Percentage = 65.00%\n",
      "Gross Accuracy = 81.37%\n",
      "Net Accuracy = 81.37%\n",
      "Detection Rate = 100.00%\n",
      "\n",
      "[INFO] serializing 113.37 encodings...\n",
      "[INFO] training model...\n",
      " \n",
      "\n",
      "[INFO] serializing 32 encodings...\n",
      "[INFO] training model...\n",
      " \n",
      "\n",
      "[INFO] serializing 32 encodings...\n",
      "[INFO] training model...\n",
      " \n",
      "\n",
      "[INFO] serializing 32 encodings...\n",
      "[INFO] training model...\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries \n",
    "from imutils import paths\n",
    "import numpy as np \n",
    "import imutils \n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC \n",
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(\n",
    "    project=\"aayushpandey616/yellowcap\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5YzU5OTY3Ni05OTQ4LTQxNTEtODFiNi1lZThlYjM1YmM4OTcifQ==\",\n",
    ")  #\n",
    "# initialize the total number of faces processed\n",
    "total = 0 # temporary variable \n",
    "populations = [52] # to calculate for what sample size\n",
    "trainpercentages = [  0.5,0.60,0.65,0.75,0.8,0.9,0.95 ] # percentage of dataset to train (out of 1)\n",
    "for population in populations:  \n",
    "    for trainpercentage in trainpercentages: \n",
    "        # the below codes are the exact replica of embedding image files  \n",
    "        protoPath = \"../face_detection_model/deploy.prototxt\"\n",
    "        modelPath = \"../face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "        detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath) \n",
    "        embedder = cv2.dnn.readNetFromTorch(\"openface_nn4.small2.v1.t7\") \n",
    "        imagePaths = list(paths.list_images(\"../dataset\")) \n",
    "        # initialize our lists of extracted facial embeddings and corresponding people names\n",
    "        knownEmbeddings = []\n",
    "        knownNames = [] \n",
    "        train = int(trainpercentage * population)\n",
    "        # loop over the image paths\n",
    "        complete = False\n",
    "        # the below code is the exact replica of the code to train the model\n",
    "        for (i, imagePath) in enumerate(imagePaths): \n",
    "            # extract the person name from the image path \n",
    "            if i == train-1: \n",
    "                break\n",
    "            name = imagePath.split(os.path.sep)[-1].split(\"-\")[-1].split(\".\")[0]\n",
    "            \n",
    "            # load the image, resize it to have a width of 600 pixels (while maintaining the aspect ratio), and then grab the image dimensions\n",
    "            image = cv2.imread(imagePath)\n",
    "            image = imutils.resize(image, width=600)\n",
    "            (h, w) = image.shape[:2] \n",
    "            # construct a blob from the image\n",
    "            imageBlob = cv2.dnn.blobFromImage(\n",
    "                cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "                (104.0, 177.0, 123.0), swapRB=False, crop=False) \n",
    "            # apply OpenCV's deep learning-based face detector to localize faces in the input image\n",
    "            detector.setInput(imageBlob)\n",
    "            detections = detector.forward()\n",
    "            # ensure at least one face was found\n",
    "            if len(detections) > 0:\n",
    "                # we're making the assumption that each image has only ONE face, so find the bounding box with the largest probability\n",
    "                i = np.argmax(detections[0, 0, :, 2])\n",
    "                confidence = detections[0, 0, i, 2] \n",
    "                # ensure that the detection with the largest probability also means our minimum probability test (thus helping filter out weak detections)\n",
    "                if confidence > 0.5:\n",
    "                    # compute the (x, y)-coordinates of the bounding box for the face\n",
    "                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                    (startX, startY, endX, endY) = box.astype(\"int\") \n",
    "                    # extract the face ROI and grab the ROI dimensions\n",
    "                    face = image[startY:endY, startX:endX]\n",
    "                    (fH, fW) = face.shape[:2] \n",
    "                    # ensure the face width and height are sufficiently large\n",
    "                    if fW < 20 or fH < 20:\n",
    "                        continue \n",
    "                    # construct a blob for the face ROI, then pass the blob through our face embedding model to obtain the 128-d quantification of the face\n",
    "                    faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                                                     (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                    embedder.setInput(faceBlob)\n",
    "                    vec = embedder.forward() \n",
    "                    # add the name of the person + corresponding face embedding to their respective lists\n",
    "                    knownNames.append(name)\n",
    "                    knownEmbeddings.append(vec.flatten())\n",
    "                    total += 1 \n",
    "        # dump the facial embeddings + names to disk\n",
    "        print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "        data = {\"embeddings\": knownEmbeddings, \"names\": knownNames} \n",
    "        # the below code is th exact replica to train the model with the embedded photos \n",
    "        # encode the labels \n",
    "        le = LabelEncoder()\n",
    "        labels = le.fit_transform(data[\"names\"]) \n",
    "        print(\"[INFO] training model...\")\n",
    "        recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "        recognizer.fit(data[\"embeddings\"], labels) \n",
    "        teststart = train \n",
    "        fullmarks = 0\n",
    "        accuracies = []  \n",
    "        # the below code is the exact replica to test/ use the trained model to identify the face  \n",
    "        for (i, imagePath) in enumerate(imagePaths):\n",
    "            if(i >= teststart and i < population):\n",
    "                fullmarks = fullmarks + 1\n",
    "                image = cv2.imread(imagePath)\n",
    "                nameactual = imagePath.split(\n",
    "                    os.path.sep)[-1].split(\"-\")[1].split(\".\")[0]\n",
    "                image = imutils.resize(image, width=600)\n",
    "                (h, w) = image.shape[:2] \n",
    "                # construct a blob from the image\n",
    "                imageBlob = cv2.dnn.blobFromImage(\n",
    "                    cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "                    (104.0, 177.0, 123.0), swapRB=False, crop=False) \n",
    "                # apply OpenCV's deep learning-based face detector to localize faces in the input image\n",
    "                detector.setInput(imageBlob)\n",
    "                detections = detector.forward() \n",
    "                # loop over the detections\n",
    "                for i in range(0, detections.shape[2]):\n",
    "                    # extract the confidence (i.e., probability) associated with the prediction\n",
    "                    confidence = detections[0, 0, i, 2]\n",
    "                    # filter out weak detections\n",
    "                    if confidence > 0.5:\n",
    "                        # compute the (x, y)-coordinates of the bounding box for the face\n",
    "                        box = detections[0, 0, i, 3:7] * \\\n",
    "                            np.array([w, h, w, h])\n",
    "                        (startX, startY, endX, endY) = box.astype(\"int\") \n",
    "                        # extract the face ROI\n",
    "                        face = image[startY:endY, startX:endX]\n",
    "                        (fH, fW) = face.shape[:2] \n",
    "                        # ensure the face width and height are sufficiently large\n",
    "                        if fW < 20 or fH < 20:\n",
    "                            continue \n",
    "                        # construct a blob for the face ROI, then pass the blob through our face embedding model to obtain the 128-d quantification of the face\n",
    "                        faceBlob = cv2.dnn.blobFromImage(\n",
    "                            face, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                        embedder.setInput(faceBlob)\n",
    "                        vec = embedder.forward() \n",
    "                        # perform classification to recognize the face\n",
    "                        preds = recognizer.predict_proba(vec)[0]\n",
    "                        j = np.argmax(preds)\n",
    "                        proba = preds[j] #probablity of prediction made by the model\n",
    "                        name = le.classes_[j] # name of the face predicted based on the trained embeddings\n",
    "                        if str(nameactual) == str(name): \n",
    "                            text = \"{:.2f}\".format(proba * 100) # formatting to nly last 2 decimal points\n",
    "                            accuracies.append(float(text)) \n",
    "        total = 0\n",
    "        for accuraci in accuracies:\n",
    "            run[\"accuracy/accuracy plot\"].log(accuraci)\n",
    "            \n",
    "            total = total + accuraci #calculating the accuracies total \n",
    "        verd = \" \\n\" # empty verdict to prevent errors\n",
    "        try:\n",
    "            verd = \"Sample = \" + str(population) + \"\\nTrain Percentage = \" + str(\"{:.2f}\".format(trainpercentage*100)) + \"%\" + \"\\nGross Accuracy = \"+str(\n",
    "                \"{:.2f}\".format(total/len(accuracies))) + \"%\\n\" + \"Net Accuracy = \" + str(\n",
    "                \"{:.2f}\".format(total/fullmarks)) + \"%\\n\" + \"Detection Rate = \"+str(\n",
    "                \"{:.2f}\".format(100 * len(accuracies)/fullmarks))+\"%\\n\" \n",
    "        except:\n",
    "            pass\n",
    "        print(verd)\n",
    "        fil = open(\"verdict.txt\", \"a\")\n",
    "        fil.write(verd)\n",
    "        fil.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "neptune": {
   "notebookId": "981a59ea-aa61-4839-a022-6e89713cff20",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
